{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "### Abstract \n",
    "\n",
    "Reinforcement Learning is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using rewards from its own actions and experiences by interacting with the environment that the agent is in.\n",
    "\n",
    "The purpose of this project is to train an agent to play the game called Frozen Lake using Q-Learning and we will get a play back of how the agent plays the game after it is trained.\n",
    "\n",
    "#### The story of Frozen Lake \n",
    "\n",
    "##### Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following\n",
    "\n",
    "<h5> SFFF<br>\n",
    "FHFH <br>\n",
    "FFFH <br>\n",
    "HFFG</h5>\n",
    "\n",
    "This grid is our environment where S is the agent’s starting point, and it’s safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that’s not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n",
    "\n",
    "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise.\n",
    "\n",
    "<h4> Steps followed in this project</h4>\n",
    "\n",
    "<h6> 1. Setting up the environment </h6>\n",
    "\n",
    "We will set up the environment, we just call gym.make() and pass a string of the name of the environment we want to set up. Here it is \"FrozenLake-v0\".\n",
    "\n",
    "<h6> 2. Creating the Q-table </h6>\n",
    "\n",
    "We will create the Q-table and initialize the Q- values to zero for each state action pair. The number of rows is equivalent to size of the state space and the number of columns is equivalent to the size of action space.\n",
    "\n",
    "<h6> 3. Initializing Q-learning parameters </h6>\n",
    "\n",
    "a. We will initialize e baseline model by the q-learning parameters<br>\n",
    "b. We will use baseline model provided by TA <br>\n",
    "c. We will tune the parameters\n",
    "\n",
    "<h6> 4. We will train the agent and check the rewards </h6>\n",
    "We will calculate the rewards after each 1000 episode and see how the agent learns from the state action pair and tries to get the maximum rewards\n",
    "<h6> 5. We will see how the agent plays withe the best hyper parameters </h6>\n",
    "\n",
    "We will wrap this by making the agent play the Frozen Lake game and run it for 3 episodes to see the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary packages needed for this project\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Frozen Lake environment where our agent will be trained and perform\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#declaring the state and action space\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "#Initializing out Q-Table with all zeroes. We will update the Q-Table after each action performed by the agent\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a model to train the agent. This will be the baseline model for the Frozen Lake environment\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.06700000000000005\n",
      "2000 :  0.21000000000000016\n",
      "3000 :  0.4090000000000003\n",
      "4000 :  0.5460000000000004\n",
      "5000 :  0.6680000000000005\n",
      "6000 :  0.6510000000000005\n",
      "7000 :  0.6740000000000005\n",
      "8000 :  0.6540000000000005\n",
      "9000 :  0.6850000000000005\n",
      "10000 :  0.6580000000000005\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "\n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "##### Baseline Performance Analysis: -\n",
    "1. The baseline performace was measured against the baseline model that we have defined in the above cells.\n",
    "   The agent was able to get more rewards when it was trained on more times. We kept the maximum number of episodes to be 10000    and maximum steps per episode to be 100. In this state the agent was able to get 67 rewards out of first 1000 episodes and      658 rewards on the last 1000 episodes.\n",
    "2. State : \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ]\n",
    "    \n",
    "    Action : LEFT = 0 DOWN = 1 RIGHT = 2 UP = 3  \n",
    "    Reward : 1 - If reaches the goal, 0 - Otherwise\n",
    "3. We chose the learning rate to be 0.1 and discount to be 0.9. <br>\n",
    "   Learning Rate - 0.1 - We want the Q-Table to update slowly and the Agent learns slowly as we have kept the number of episodes    as 10000 which is pretty large. <br>\n",
    "   Discount Factor - 0.9 - We want the agent to consider the future reward rather than focussing on the immediate rewards. \n",
    "\n",
    "##### Now lets change the learning rate and the discount factor and see how the rewards change for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the alpha and Gamma i.e the learning Rate and the discount factor and assessing how tha agent plays\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.3 # alpha\n",
    "discount_rate = 0.8 # gamma\n",
    "\n",
    "exploration_rate = 1 # epsilon\n",
    "max_exploration_rate = 1 #max epsilon\n",
    "min_exploration_rate = 0.01 #min epsilon\n",
    "exploration_decay_rate = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.03300000000000002\n",
      "2000 :  0.09200000000000007\n",
      "3000 :  0.09800000000000007\n",
      "4000 :  0.17500000000000013\n",
      "5000 :  0.2580000000000002\n",
      "6000 :  0.20100000000000015\n",
      "7000 :  0.3090000000000002\n",
      "8000 :  0.33800000000000024\n",
      "9000 :  0.35800000000000026\n",
      "10000 :  0.3110000000000002\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "\n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance Analysis after tweak: -\n",
    "\n",
    "1. We changed the alpha (learning Rate) and gamma (discount factor) and trained the agent. The rewards changed drastically. <br>\n",
    "   As the learning rate is changed from 0.1 to 0.3 the agent started learning fast and as the discount is changed from 0.9 to      0.8 it started focussing on the immediate rewards more than future, so the rewards gained in the initial trainings were more    than the changes in the rewards in the later trainings.<br>\n",
    "2. The starting epsilon is kept fixed at 0.01 and the max epsilon is 1 which are just the bounds for the epsilon which is          1 i.e (100%) because we should be certain that our agent will start exploring the environment.<br>\n",
    "   The decay rate (exploitation) is set as low as possible to 0.001, so that we want to make sure that the agent will stick to      explore the environment rather than exploiting i.e. not choosing the action depending on the highest Q- value rather than        randomly choosing action and exploring what happens in the environment.<br>\n",
    "3. Average number of steps taken per episode - We will calculate that when we see our agent play in the next step.<br>\n",
    "4. Q-Learning is the value based iteration because we choose the action that determines the maximum rewards. <br>\n",
    "\n",
    "##### We will now use the baseline provided and determine how the agent beaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Baseline model provided for training the agent\n",
    "total_episodes = 5000\n",
    "total_test_episodes = 100\n",
    "max_steps = 99\n",
    "alpha= 0.7 # Learning rate\n",
    "gamma = 0.8 # Discounting rate\n",
    "epsilon = 1.0 # Exploration rate\n",
    "decay_rate = 0.01 # Exponential decay rate\n",
    "\n",
    "\n",
    "## Assigning the given baseline to our variable\n",
    "\n",
    "num_episodes = total_episodes\n",
    "max_steps_per_episode = max_steps\n",
    "\n",
    "learning_rate = alpha\n",
    "discount_rate = gamma\n",
    "\n",
    "exploration_rate = epsilon\n",
    "max_exploration_rate = epsilon*1\n",
    "min_exploration_rate = epsilon * 0.01\n",
    "exploration_decay_rate = decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.20800000000000016\n",
      "2000 :  0.32800000000000024\n",
      "3000 :  0.34600000000000025\n",
      "4000 :  0.2710000000000002\n",
      "5000 :  0.35700000000000026\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "\n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters tuning\n",
    "We will now tune the hyper parameter by tuning the alpha, gamma, epsilon, decay rate and the number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 200\n",
    "\n",
    "learning_rate = 0.1 # alpha\n",
    "discount_rate = 0.99 # gamma\n",
    "\n",
    "exploration_rate = 1 # epsilon\n",
    "max_exploration_rate = 1 #max epsilon\n",
    "min_exploration_rate = 0.01 #min epsilon\n",
    "exploration_decay_rate = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.046000000000000034\n",
      "2000 :  0.21300000000000016\n",
      "3000 :  0.44500000000000034\n",
      "4000 :  0.5750000000000004\n",
      "5000 :  0.6170000000000004\n",
      "6000 :  0.6950000000000005\n",
      "7000 :  0.6860000000000005\n",
      "8000 :  0.6890000000000005\n",
      "9000 :  0.6990000000000005\n",
      "10000 :  0.6820000000000005\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "\n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the model - Making Agent Play\n",
    "\n",
    "Now we have trained our agent and tweaked the hyper parameters to find out the best reward the agent can generate. <br><br>\n",
    "As we have already stated that the agent will try to get the maximum rewards and try to reach the goal in less numebr of steps. <br><br>\n",
    "##### Now lets see how the agent plays the Frozen Lake game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n",
      "Number of steps taken  32\n"
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                # Agent reached the goal and won episode\n",
    "                print(\"****You reached the goal!****\")\n",
    "                print(\"Number of steps taken \",step)\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                # Agent stepped in a hole and lost episode     \n",
    "                print(\"****You fell through a hole!****\")\n",
    "                print(\"Number of steps taken \",step)\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            break\n",
    "\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "So we have trained our agent with baseline parameters and then tuned the hyper parameters.\n",
    "We have thenmade the agent play the Frozen Lake and then captured the result. We can see that the agent has learned the game well enough to play the game and in our test run it has successfully reached the goal within 32 steps.\n",
    "So we can say that the agent learned to play the game from his action and the rewards that he receined after making the correct move and reaching the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "1. https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "2. https://github.com/openai/gym\n",
    "3. https://deeplizard.com/learn/video/QK_PP_2KgGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "Copyright 2020 ANURAG DHAR\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
